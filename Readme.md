
---

### Extending GPT-2 Context Length via RoPE Scaling

#### Training Runs
- [Training Run 1 on WandB](https://wandb.ai/dumbal/huggingface/runs/omafkp4r?nw=nwuserdumbal)
- [Training Run 2 on WandB](https://wandb.ai/dumbal/huggingface/runs/pivwo4nb?nw=nwuserdumbal)

#### Demo
- Try the model here: [GPT-2 Long Demo](https://huggingface.co/spaces/archit11/gpt2long)

![Demo](./image.png)

#### Evaluation
- Not as good as expected: [RoPE Test Evaluation](https://github.com/kaiokendev/cutoff-len-is-context-len/blob/main/rope_test.ipynb)
 but...good enough! given the compute constrians! :D

---

