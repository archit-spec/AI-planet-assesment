# extending gpt2 ctx len via rope scaling

training run -> [https://wandb.ai/duhttps://huggingface.co/spaces/archit11/gpt2longmbal/huggingface/runs/omafkp4r?nw=nwuserdumbal](wandb1)
training run2 -> [https://wandb.ai/dumbal/huggingface/runs/pivwo4nb?nw=nwuserdumbal](wandb2)

# ok so it kind of works try here [https://huggingface.co/spaces/archit11/gpt2long](demo)

 [demo](./image.png)
not as good this [https://github.com/kaiokendev/cutoff-len-is-context-len/blob/main/rope_test.ipynb](eval) 

